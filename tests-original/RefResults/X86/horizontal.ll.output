; ModuleID = '<stdin>'
source_filename = "<stdin>"
target triple = "x86_64-apple-macosx"

@arr_i32 = global [32 x i32] zeroinitializer, align 16
@arr_float = global [32 x float] zeroinitializer, align 16

define i32 @add_red(float* %A, i32 %n) #0 {
entry:
  %cmp31 = icmp sgt i32 %n, 0
  br i1 %cmp31, label %for.body.lr.ph, label %for.end

for.body.lr.ph:                                   ; preds = %entry
  %0 = sext i32 %n to i64
  br label %for.body

for.body:                                         ; preds = %for.body, %for.body.lr.ph
  %i.033 = phi i64 [ 0, %for.body.lr.ph ], [ %inc, %for.body ]
  %sum.032 = phi float [ 0.000000e+00, %for.body.lr.ph ], [ %add17, %for.body ]
  %mul = shl nsw i64 %i.033, 2
  %arrayidx = getelementptr inbounds float, float* %A, i64 %mul
  %add28 = or i64 %mul, 1
  %arrayidx4 = getelementptr inbounds float, float* %A, i64 %add28
  %add829 = or i64 %mul, 2
  %arrayidx9 = getelementptr inbounds float, float* %A, i64 %add829
  %add1330 = or i64 %mul, 3
  %arrayidx14 = getelementptr inbounds float, float* %A, i64 %add1330
  %1 = bitcast float* %arrayidx to <4 x float>*
  %2 = load <4 x float>, <4 x float>* %1, align 4
  %3 = fmul <4 x float> <float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00>, %2
  %add6 = fadd fast float undef, undef
  %add11 = fadd fast float %add6, undef
  %rdx.shuf = shufflevector <4 x float> %3, <4 x float> undef, <4 x i32> <i32 2, i32 3, i32 undef, i32 undef>
  %bin.rdx = fadd fast <4 x float> %3, %rdx.shuf
  %rdx.shuf1 = shufflevector <4 x float> %bin.rdx, <4 x float> undef, <4 x i32> <i32 1, i32 undef, i32 undef, i32 undef>
  %bin.rdx2 = fadd fast <4 x float> %bin.rdx, %rdx.shuf1
  %4 = extractelement <4 x float> %bin.rdx2, i32 0
  %add16 = fadd fast float %add11, undef
  %add17 = fadd fast float %sum.032, %4
  %inc = add nsw i64 %i.033, 1
  %exitcond = icmp eq i64 %inc, %0
  br i1 %exitcond, label %for.cond.for.end_crit_edge, label %for.body

for.cond.for.end_crit_edge:                       ; preds = %for.body
  %phitmp = fptosi float %add17 to i32
  br label %for.end

for.end:                                          ; preds = %for.cond.for.end_crit_edge, %entry
  %sum.0.lcssa = phi i32 [ %phitmp, %for.cond.for.end_crit_edge ], [ 0, %entry ]
  ret i32 %sum.0.lcssa
}

define i32 @mul_red(float* noalias %A, float* noalias %B, i32 %n) #0 {
entry:
  %cmp38 = icmp sgt i32 %n, 0
  br i1 %cmp38, label %for.body.lr.ph, label %for.end

for.body.lr.ph:                                   ; preds = %entry
  %arrayidx4 = getelementptr inbounds float, float* %B, i64 1
  %arrayidx9 = getelementptr inbounds float, float* %B, i64 2
  %arrayidx15 = getelementptr inbounds float, float* %B, i64 3
  %0 = bitcast float* %B to <4 x float>*
  %1 = load <4 x float>, <4 x float>* %0, align 4
  %2 = sext i32 %n to i64
  br label %for.body

for.body:                                         ; preds = %for.body, %for.body.lr.ph
  %i.040 = phi i64 [ 0, %for.body.lr.ph ], [ %inc, %for.body ]
  %sum.039 = phi float [ 0.000000e+00, %for.body.lr.ph ], [ %mul21, %for.body ]
  %mul = shl nsw i64 %i.040, 2
  %arrayidx2 = getelementptr inbounds float, float* %A, i64 %mul
  %add35 = or i64 %mul, 1
  %arrayidx6 = getelementptr inbounds float, float* %A, i64 %add35
  %add1136 = or i64 %mul, 2
  %arrayidx12 = getelementptr inbounds float, float* %A, i64 %add1136
  %add1737 = or i64 %mul, 3
  %arrayidx18 = getelementptr inbounds float, float* %A, i64 %add1737
  %3 = bitcast float* %arrayidx2 to <4 x float>*
  %4 = load <4 x float>, <4 x float>* %3, align 4
  %5 = fmul <4 x float> %1, %4
  %add8 = fadd fast float undef, undef
  %add14 = fadd fast float %add8, undef
  %rdx.shuf = shufflevector <4 x float> %5, <4 x float> undef, <4 x i32> <i32 2, i32 3, i32 undef, i32 undef>
  %bin.rdx = fadd fast <4 x float> %5, %rdx.shuf
  %rdx.shuf1 = shufflevector <4 x float> %bin.rdx, <4 x float> undef, <4 x i32> <i32 1, i32 undef, i32 undef, i32 undef>
  %bin.rdx2 = fadd fast <4 x float> %bin.rdx, %rdx.shuf1
  %6 = extractelement <4 x float> %bin.rdx2, i32 0
  %add20 = fadd fast float %add14, undef
  %mul21 = fmul float %sum.039, %6
  %inc = add nsw i64 %i.040, 1
  %exitcond = icmp eq i64 %inc, %2
  br i1 %exitcond, label %for.cond.for.end_crit_edge, label %for.body

for.cond.for.end_crit_edge:                       ; preds = %for.body
  %phitmp = fptosi float %mul21 to i32
  br label %for.end

for.end:                                          ; preds = %for.cond.for.end_crit_edge, %entry
  %sum.0.lcssa = phi i32 [ %phitmp, %for.cond.for.end_crit_edge ], [ 0, %entry ]
  ret i32 %sum.0.lcssa
}

define i32 @long_red(float* noalias %A, float* noalias %B, i32 %n) #0 {
entry:
  %cmp81 = icmp sgt i32 %n, 0
  br i1 %cmp81, label %for.body.lr.ph, label %for.end

for.body.lr.ph:                                   ; preds = %entry
  %arrayidx4 = getelementptr inbounds float, float* %B, i64 1
  %arrayidx9 = getelementptr inbounds float, float* %B, i64 2
  %arrayidx15 = getelementptr inbounds float, float* %B, i64 3
  %arrayidx21 = getelementptr inbounds float, float* %B, i64 4
  %arrayidx27 = getelementptr inbounds float, float* %B, i64 5
  %arrayidx33 = getelementptr inbounds float, float* %B, i64 6
  %arrayidx39 = getelementptr inbounds float, float* %B, i64 7
  %0 = bitcast float* %B to <8 x float>*
  %1 = load <8 x float>, <8 x float>* %0, align 4
  %arrayidx45 = getelementptr inbounds float, float* %B, i64 8
  %2 = load float, float* %arrayidx45, align 4
  %3 = sext i32 %n to i64
  br label %for.body

for.body:                                         ; preds = %for.body, %for.body.lr.ph
  %i.083 = phi i64 [ 0, %for.body.lr.ph ], [ %inc, %for.body ]
  %sum.082 = phi float [ 0.000000e+00, %for.body.lr.ph ], [ %add51, %for.body ]
  %mul = mul nsw i64 %i.083, 6
  %arrayidx2 = getelementptr inbounds float, float* %A, i64 %mul
  %add80 = or i64 %mul, 1
  %arrayidx6 = getelementptr inbounds float, float* %A, i64 %add80
  %add11 = add nsw i64 %mul, 2
  %arrayidx12 = getelementptr inbounds float, float* %A, i64 %add11
  %add17 = add nsw i64 %mul, 3
  %arrayidx18 = getelementptr inbounds float, float* %A, i64 %add17
  %add23 = add nsw i64 %mul, 4
  %arrayidx24 = getelementptr inbounds float, float* %A, i64 %add23
  %add29 = add nsw i64 %mul, 5
  %arrayidx30 = getelementptr inbounds float, float* %A, i64 %add29
  %add35 = add nsw i64 %mul, 6
  %arrayidx36 = getelementptr inbounds float, float* %A, i64 %add35
  %add41 = add nsw i64 %mul, 7
  %arrayidx42 = getelementptr inbounds float, float* %A, i64 %add41
  %4 = bitcast float* %arrayidx2 to <8 x float>*
  %5 = load <8 x float>, <8 x float>* %4, align 4
  %6 = fmul fast <8 x float> %1, %5
  %add8 = fadd fast float undef, undef
  %add14 = fadd fast float %add8, undef
  %add20 = fadd fast float %add14, undef
  %add26 = fadd fast float %add20, undef
  %add32 = fadd fast float %add26, undef
  %add38 = fadd fast float %add32, undef
  %add44 = fadd fast float %add38, undef
  %add47 = add nsw i64 %mul, 8
  %arrayidx48 = getelementptr inbounds float, float* %A, i64 %add47
  %7 = load float, float* %arrayidx48, align 4
  %mul49 = fmul fast float %2, %7
  %rdx.shuf = shufflevector <8 x float> %6, <8 x float> undef, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 undef, i32 undef, i32 undef, i32 undef>
  %bin.rdx = fadd fast <8 x float> %6, %rdx.shuf
  %rdx.shuf1 = shufflevector <8 x float> %bin.rdx, <8 x float> undef, <8 x i32> <i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %bin.rdx2 = fadd fast <8 x float> %bin.rdx, %rdx.shuf1
  %rdx.shuf3 = shufflevector <8 x float> %bin.rdx2, <8 x float> undef, <8 x i32> <i32 1, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %bin.rdx4 = fadd fast <8 x float> %bin.rdx2, %rdx.shuf3
  %8 = extractelement <8 x float> %bin.rdx4, i32 0
  %9 = fadd fast float %8, %mul49
  %add50 = fadd fast float %add44, %mul49
  %add51 = fadd fast float %sum.082, %9
  %inc = add nsw i64 %i.083, 1
  %exitcond = icmp eq i64 %inc, %3
  br i1 %exitcond, label %for.cond.for.end_crit_edge, label %for.body

for.cond.for.end_crit_edge:                       ; preds = %for.body
  %phitmp = fptosi float %add51 to i32
  br label %for.end

for.end:                                          ; preds = %for.cond.for.end_crit_edge, %entry
  %sum.0.lcssa = phi i32 [ %phitmp, %for.cond.for.end_crit_edge ], [ 0, %entry ]
  ret i32 %sum.0.lcssa
}

define i32 @chain_red(float* noalias %A, float* noalias %B, i32 %n) #0 {
entry:
  %cmp41 = icmp sgt i32 %n, 0
  br i1 %cmp41, label %for.body.lr.ph, label %for.end

for.body.lr.ph:                                   ; preds = %entry
  %arrayidx4 = getelementptr inbounds float, float* %B, i64 1
  %arrayidx10 = getelementptr inbounds float, float* %B, i64 2
  %arrayidx16 = getelementptr inbounds float, float* %B, i64 3
  %0 = bitcast float* %B to <4 x float>*
  %1 = load <4 x float>, <4 x float>* %0, align 4
  %2 = sext i32 %n to i64
  br label %for.body

for.body:                                         ; preds = %for.body, %for.body.lr.ph
  %i.043 = phi i64 [ 0, %for.body.lr.ph ], [ %inc, %for.body ]
  %sum.042 = phi float [ 0.000000e+00, %for.body.lr.ph ], [ %op.extra, %for.body ]
  %mul = shl nsw i64 %i.043, 2
  %arrayidx2 = getelementptr inbounds float, float* %A, i64 %mul
  %add638 = or i64 %mul, 1
  %arrayidx7 = getelementptr inbounds float, float* %A, i64 %add638
  %add1239 = or i64 %mul, 2
  %arrayidx13 = getelementptr inbounds float, float* %A, i64 %add1239
  %add1840 = or i64 %mul, 3
  %arrayidx19 = getelementptr inbounds float, float* %A, i64 %add1840
  %3 = bitcast float* %arrayidx2 to <4 x float>*
  %4 = load <4 x float>, <4 x float>* %3, align 4
  %5 = fmul fast <4 x float> %1, %4
  %add = fadd fast float %sum.042, undef
  %add9 = fadd fast float %add, undef
  %add15 = fadd fast float %add9, undef
  %rdx.shuf = shufflevector <4 x float> %5, <4 x float> undef, <4 x i32> <i32 2, i32 3, i32 undef, i32 undef>
  %bin.rdx = fadd fast <4 x float> %5, %rdx.shuf
  %rdx.shuf1 = shufflevector <4 x float> %bin.rdx, <4 x float> undef, <4 x i32> <i32 1, i32 undef, i32 undef, i32 undef>
  %bin.rdx2 = fadd fast <4 x float> %bin.rdx, %rdx.shuf1
  %6 = extractelement <4 x float> %bin.rdx2, i32 0
  %op.extra = fadd fast float %6, %sum.042
  %add21 = fadd fast float %add15, undef
  %inc = add nsw i64 %i.043, 1
  %exitcond = icmp eq i64 %inc, %2
  br i1 %exitcond, label %for.cond.for.end_crit_edge, label %for.body

for.cond.for.end_crit_edge:                       ; preds = %for.body
  %phitmp = fptosi float %op.extra to i32
  br label %for.end

for.end:                                          ; preds = %for.cond.for.end_crit_edge, %entry
  %sum.0.lcssa = phi i32 [ %phitmp, %for.cond.for.end_crit_edge ], [ 0, %entry ]
  ret i32 %sum.0.lcssa
}

define void @foo(float* nocapture readonly %arg_A, i32 %arg_B, float* nocapture %array) #0 {
entry:
  %cmp1495 = icmp eq i32 %arg_B, 0
  br label %for.body

for.cond.cleanup:                                 ; preds = %for.cond.cleanup15
  ret void

for.body:                                         ; preds = %for.cond.cleanup15, %entry
  %indvars.iv = phi i64 [ 0, %entry ], [ %indvars.iv.next, %for.cond.cleanup15 ]
  %0 = shl i64 %indvars.iv, 2
  %arrayidx = getelementptr inbounds float, float* %array, i64 %0
  %1 = load float, float* %arrayidx, align 4
  %2 = or i64 %0, 1
  %arrayidx4 = getelementptr inbounds float, float* %array, i64 %2
  %3 = load float, float* %arrayidx4, align 4
  %4 = or i64 %0, 2
  %arrayidx8 = getelementptr inbounds float, float* %array, i64 %4
  %5 = load float, float* %arrayidx8, align 4
  %6 = or i64 %0, 3
  %arrayidx12 = getelementptr inbounds float, float* %array, i64 %6
  %7 = load float, float* %arrayidx12, align 4
  br i1 %cmp1495, label %for.cond.cleanup15, label %for.body16.lr.ph

for.body16.lr.ph:                                 ; preds = %for.body
  %add.ptr = getelementptr inbounds float, float* %arg_A, i64 %indvars.iv
  %8 = load float, float* %add.ptr, align 4
  br label %for.body16

for.cond.cleanup15:                               ; preds = %for.body16, %for.body
  %w2.0.lcssa = phi float [ %5, %for.body ], [ %sub28, %for.body16 ]
  %w3.0.lcssa = phi float [ %7, %for.body ], [ %w2.096, %for.body16 ]
  %w1.0.lcssa = phi float [ %3, %for.body ], [ %w0.0100, %for.body16 ]
  %w0.0.lcssa = phi float [ %1, %for.body ], [ %sub19, %for.body16 ]
  store float %w0.0.lcssa, float* %arrayidx, align 4
  store float %w1.0.lcssa, float* %arrayidx4, align 4
  store float %w2.0.lcssa, float* %arrayidx8, align 4
  store float %w3.0.lcssa, float* %arrayidx12, align 4
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond109 = icmp eq i64 %indvars.iv.next, 6
  br i1 %exitcond109, label %for.cond.cleanup, label %for.body

for.body16:                                       ; preds = %for.body16, %for.body16.lr.ph
  %w0.0100 = phi float [ %1, %for.body16.lr.ph ], [ %sub19, %for.body16 ]
  %w1.099 = phi float [ %3, %for.body16.lr.ph ], [ %w0.0100, %for.body16 ]
  %j.098 = phi i32 [ 0, %for.body16.lr.ph ], [ %inc, %for.body16 ]
  %w3.097 = phi float [ %7, %for.body16.lr.ph ], [ %w2.096, %for.body16 ]
  %w2.096 = phi float [ %5, %for.body16.lr.ph ], [ %sub28, %for.body16 ]
  %mul17 = fmul fast float %w0.0100, 0x3FF19999A0000000
  %mul18.neg = fmul fast float %w1.099, 0xBFF3333340000000
  %sub92 = fadd fast float %mul17, %mul18.neg
  %sub19 = fadd fast float %sub92, %8
  %mul20 = fmul fast float %sub19, 0x4000CCCCC0000000
  %mul21.neg = fmul fast float %w0.0100, 0xC0019999A0000000
  %mul23 = fmul fast float %w1.099, 0x4002666660000000
  %mul25 = fmul fast float %w2.096, 0x4008CCCCC0000000
  %mul27.neg = fmul fast float %w3.097, 0xC0099999A0000000
  %add2293 = fadd fast float %mul27.neg, %mul25
  %add24 = fadd fast float %add2293, %mul23
  %sub2694 = fadd fast float %add24, %mul21.neg
  %sub28 = fadd fast float %sub2694, %mul20
  %inc = add nuw i32 %j.098, 1
  %exitcond = icmp eq i32 %inc, %arg_B
  br i1 %exitcond, label %for.cond.cleanup15, label %for.body16
}

define void @store_red_double(double* noalias %A, double* noalias %B, double* noalias %C, i32 %n) #0 {
entry:
  %cmp17 = icmp sgt i32 %n, 0
  br i1 %cmp17, label %for.body.lr.ph, label %for.end

for.body.lr.ph:                                   ; preds = %entry
  %0 = load double, double* %B, align 8
  %arrayidx4 = getelementptr inbounds double, double* %B, i64 1
  %1 = load double, double* %arrayidx4, align 8
  %2 = sext i32 %n to i64
  br label %for.body

for.body:                                         ; preds = %for.body, %for.body.lr.ph
  %i.018 = phi i64 [ 0, %for.body.lr.ph ], [ %inc, %for.body ]
  %mul = shl nsw i64 %i.018, 2
  %arrayidx2 = getelementptr inbounds double, double* %A, i64 %mul
  %3 = load double, double* %arrayidx2, align 8
  %mul3 = fmul fast double %0, %3
  %add16 = or i64 %mul, 1
  %arrayidx6 = getelementptr inbounds double, double* %A, i64 %add16
  %4 = load double, double* %arrayidx6, align 8
  %mul7 = fmul fast double %1, %4
  %add8 = fadd fast double %mul3, %mul7
  %arrayidx9 = getelementptr inbounds double, double* %C, i64 %i.018
  store double %add8, double* %arrayidx9, align 8
  %inc = add nsw i64 %i.018, 1
  %exitcond = icmp eq i64 %inc, %2
  br i1 %exitcond, label %for.end, label %for.body

for.end:                                          ; preds = %for.body, %entry
  ret void
}

define i32 @store_red(float* noalias %A, float* noalias %B, float* noalias %C, i32 %n) #0 {
entry:
  %cmp37 = icmp sgt i32 %n, 0
  br i1 %cmp37, label %for.body.lr.ph, label %for.end

for.body.lr.ph:                                   ; preds = %entry
  %arrayidx4 = getelementptr inbounds float, float* %B, i64 1
  %arrayidx9 = getelementptr inbounds float, float* %B, i64 2
  %arrayidx15 = getelementptr inbounds float, float* %B, i64 3
  %0 = sext i32 %n to i64
  br label %for.body

for.body:                                         ; preds = %for.body, %for.body.lr.ph
  %i.039 = phi i64 [ 0, %for.body.lr.ph ], [ %inc, %for.body ]
  %C.addr.038 = phi float* [ %C, %for.body.lr.ph ], [ %incdec.ptr, %for.body ]
  %1 = load float, float* %B, align 4
  %mul = shl nsw i64 %i.039, 2
  %arrayidx2 = getelementptr inbounds float, float* %A, i64 %mul
  %2 = load float, float* %arrayidx2, align 4
  %mul3 = fmul fast float %1, %2
  %3 = load float, float* %arrayidx4, align 4
  %add34 = or i64 %mul, 1
  %arrayidx6 = getelementptr inbounds float, float* %A, i64 %add34
  %4 = load float, float* %arrayidx6, align 4
  %mul7 = fmul fast float %3, %4
  %add8 = fadd fast float %mul3, %mul7
  %5 = load float, float* %arrayidx9, align 4
  %add1135 = or i64 %mul, 2
  %arrayidx12 = getelementptr inbounds float, float* %A, i64 %add1135
  %6 = load float, float* %arrayidx12, align 4
  %mul13 = fmul fast float %5, %6
  %add14 = fadd fast float %add8, %mul13
  %7 = load float, float* %arrayidx15, align 4
  %add1736 = or i64 %mul, 3
  %arrayidx18 = getelementptr inbounds float, float* %A, i64 %add1736
  %8 = load float, float* %arrayidx18, align 4
  %mul19 = fmul fast float %7, %8
  %add20 = fadd fast float %add14, %mul19
  store float %add20, float* %C.addr.038, align 4
  %incdec.ptr = getelementptr inbounds float, float* %C.addr.038, i64 1
  %inc = add nsw i64 %i.039, 1
  %exitcond = icmp eq i64 %inc, %0
  br i1 %exitcond, label %for.end, label %for.body

for.end:                                          ; preds = %for.body, %entry
  ret i32 0
}

define void @float_red_example4(float* %res) #0 {
entry:
  %0 = load float, float* getelementptr inbounds ([32 x float], [32 x float]* @arr_float, i64 0, i64 0), align 16
  %1 = load float, float* getelementptr inbounds ([32 x float], [32 x float]* @arr_float, i64 0, i64 1), align 4
  %add = fadd fast float %1, %0
  %2 = load float, float* getelementptr inbounds ([32 x float], [32 x float]* @arr_float, i64 0, i64 2), align 8
  %add.1 = fadd fast float %2, %add
  %3 = load float, float* getelementptr inbounds ([32 x float], [32 x float]* @arr_float, i64 0, i64 3), align 4
  %add.2 = fadd fast float %3, %add.1
  store float %add.2, float* %res, align 16
  ret void
}

define void @float_red_example8(float* %res) #0 {
entry:
  %0 = load float, float* getelementptr inbounds ([32 x float], [32 x float]* @arr_float, i64 0, i64 0), align 16
  %1 = load float, float* getelementptr inbounds ([32 x float], [32 x float]* @arr_float, i64 0, i64 1), align 4
  %add = fadd fast float %1, %0
  %2 = load float, float* getelementptr inbounds ([32 x float], [32 x float]* @arr_float, i64 0, i64 2), align 8
  %add.1 = fadd fast float %2, %add
  %3 = load float, float* getelementptr inbounds ([32 x float], [32 x float]* @arr_float, i64 0, i64 3), align 4
  %add.2 = fadd fast float %3, %add.1
  %4 = load float, float* getelementptr inbounds ([32 x float], [32 x float]* @arr_float, i64 0, i64 4), align 16
  %add.3 = fadd fast float %4, %add.2
  %5 = load float, float* getelementptr inbounds ([32 x float], [32 x float]* @arr_float, i64 0, i64 5), align 4
  %add.4 = fadd fast float %5, %add.3
  %6 = load float, float* getelementptr inbounds ([32 x float], [32 x float]* @arr_float, i64 0, i64 6), align 8
  %add.5 = fadd fast float %6, %add.4
  %7 = load float, float* getelementptr inbounds ([32 x float], [32 x float]* @arr_float, i64 0, i64 7), align 4
  %add.6 = fadd fast float %7, %add.5
  store float %add.6, float* %res, align 16
  ret void
}

define void @float_red_example16(float* %res) #0 {
entry:
  %0 = load float, float* getelementptr inbounds ([32 x float], [32 x float]* @arr_float, i64 0, i64 0), align 16
  %1 = load float, float* getelementptr inbounds ([32 x float], [32 x float]* @arr_float, i64 0, i64 1), align 4
  %add = fadd fast float %1, %0
  %2 = load float, float* getelementptr inbounds ([32 x float], [32 x float]* @arr_float, i64 0, i64 2), align 8
  %add.1 = fadd fast float %2, %add
  %3 = load float, float* getelementptr inbounds ([32 x float], [32 x float]* @arr_float, i64 0, i64 3), align 4
  %add.2 = fadd fast float %3, %add.1
  %4 = load float, float* getelementptr inbounds ([32 x float], [32 x float]* @arr_float, i64 0, i64 4), align 16
  %add.3 = fadd fast float %4, %add.2
  %5 = load float, float* getelementptr inbounds ([32 x float], [32 x float]* @arr_float, i64 0, i64 5), align 4
  %add.4 = fadd fast float %5, %add.3
  %6 = load float, float* getelementptr inbounds ([32 x float], [32 x float]* @arr_float, i64 0, i64 6), align 8
  %add.5 = fadd fast float %6, %add.4
  %7 = load float, float* getelementptr inbounds ([32 x float], [32 x float]* @arr_float, i64 0, i64 7), align 4
  %add.6 = fadd fast float %7, %add.5
  %8 = load float, float* getelementptr inbounds ([32 x float], [32 x float]* @arr_float, i64 0, i64 8), align 16
  %add.7 = fadd fast float %8, %add.6
  %9 = load float, float* getelementptr inbounds ([32 x float], [32 x float]* @arr_float, i64 0, i64 9), align 4
  %add.8 = fadd fast float %9, %add.7
  %10 = load float, float* getelementptr inbounds ([32 x float], [32 x float]* @arr_float, i64 0, i64 10), align 8
  %add.9 = fadd fast float %10, %add.8
  %11 = load float, float* getelementptr inbounds ([32 x float], [32 x float]* @arr_float, i64 0, i64 11), align 4
  %add.10 = fadd fast float %11, %add.9
  %12 = load float, float* getelementptr inbounds ([32 x float], [32 x float]* @arr_float, i64 0, i64 12), align 16
  %add.11 = fadd fast float %12, %add.10
  %13 = load float, float* getelementptr inbounds ([32 x float], [32 x float]* @arr_float, i64 0, i64 13), align 4
  %add.12 = fadd fast float %13, %add.11
  %14 = load float, float* getelementptr inbounds ([32 x float], [32 x float]* @arr_float, i64 0, i64 14), align 8
  %add.13 = fadd fast float %14, %add.12
  %15 = load float, float* getelementptr inbounds ([32 x float], [32 x float]* @arr_float, i64 0, i64 15), align 4
  %add.14 = fadd fast float %15, %add.13
  store float %add.14, float* %res, align 16
  ret void
}

define void @i32_red_example4(i32* %res) #0 {
entry:
  %0 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 0), align 16
  %1 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 1), align 4
  %add = add nsw i32 %1, %0
  %2 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 2), align 8
  %add.1 = add nsw i32 %2, %add
  %3 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 3), align 4
  %add.2 = add nsw i32 %3, %add.1
  store i32 %add.2, i32* %res, align 16
  ret void
}

define void @i32_red_example8(i32* %res) #0 {
entry:
  %0 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 0), align 16
  %1 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 1), align 4
  %add = add nsw i32 %1, %0
  %2 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 2), align 8
  %add.1 = add nsw i32 %2, %add
  %3 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 3), align 4
  %add.2 = add nsw i32 %3, %add.1
  %4 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 4), align 16
  %add.3 = add nsw i32 %4, %add.2
  %5 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 5), align 4
  %add.4 = add nsw i32 %5, %add.3
  %6 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 6), align 8
  %add.5 = add nsw i32 %6, %add.4
  %7 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 7), align 4
  %add.6 = add nsw i32 %7, %add.5
  store i32 %add.6, i32* %res, align 16
  ret void
}

define void @i32_red_example16(i32* %res) #0 {
entry:
  %0 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 0), align 16
  %1 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 1), align 4
  %add = add nsw i32 %1, %0
  %2 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 2), align 8
  %add.1 = add nsw i32 %2, %add
  %3 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 3), align 4
  %add.2 = add nsw i32 %3, %add.1
  %4 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 4), align 16
  %add.3 = add nsw i32 %4, %add.2
  %5 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 5), align 4
  %add.4 = add nsw i32 %5, %add.3
  %6 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 6), align 8
  %add.5 = add nsw i32 %6, %add.4
  %7 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 7), align 4
  %add.6 = add nsw i32 %7, %add.5
  %8 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 8), align 16
  %add.7 = add nsw i32 %8, %add.6
  %9 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 9), align 4
  %add.8 = add nsw i32 %9, %add.7
  %10 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 10), align 8
  %add.9 = add nsw i32 %10, %add.8
  %11 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 11), align 4
  %add.10 = add nsw i32 %11, %add.9
  %12 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 12), align 16
  %add.11 = add nsw i32 %12, %add.10
  %13 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 13), align 4
  %add.12 = add nsw i32 %13, %add.11
  %14 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 14), align 8
  %add.13 = add nsw i32 %14, %add.12
  %15 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 15), align 4
  %add.14 = add nsw i32 %15, %add.13
  store i32 %add.14, i32* %res, align 16
  ret void
}

define void @i32_red_example32(i32* %res) #0 {
entry:
  %0 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 0), align 16
  %1 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 1), align 4
  %add = add nsw i32 %1, %0
  %2 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 2), align 8
  %add.1 = add nsw i32 %2, %add
  %3 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 3), align 4
  %add.2 = add nsw i32 %3, %add.1
  %4 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 4), align 16
  %add.3 = add nsw i32 %4, %add.2
  %5 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 5), align 4
  %add.4 = add nsw i32 %5, %add.3
  %6 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 6), align 8
  %add.5 = add nsw i32 %6, %add.4
  %7 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 7), align 4
  %add.6 = add nsw i32 %7, %add.5
  %8 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 8), align 16
  %add.7 = add nsw i32 %8, %add.6
  %9 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 9), align 4
  %add.8 = add nsw i32 %9, %add.7
  %10 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 10), align 8
  %add.9 = add nsw i32 %10, %add.8
  %11 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 11), align 4
  %add.10 = add nsw i32 %11, %add.9
  %12 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 12), align 16
  %add.11 = add nsw i32 %12, %add.10
  %13 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 13), align 4
  %add.12 = add nsw i32 %13, %add.11
  %14 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 14), align 8
  %add.13 = add nsw i32 %14, %add.12
  %15 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 15), align 4
  %add.14 = add nsw i32 %15, %add.13
  %16 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 16), align 16
  %add.15 = add nsw i32 %16, %add.14
  %17 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 17), align 4
  %add.16 = add nsw i32 %17, %add.15
  %18 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 18), align 8
  %add.17 = add nsw i32 %18, %add.16
  %19 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 19), align 4
  %add.18 = add nsw i32 %19, %add.17
  %20 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 20), align 16
  %add.19 = add nsw i32 %20, %add.18
  %21 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 21), align 4
  %add.20 = add nsw i32 %21, %add.19
  %22 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 22), align 8
  %add.21 = add nsw i32 %22, %add.20
  %23 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 23), align 4
  %add.22 = add nsw i32 %23, %add.21
  %24 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 24), align 16
  %add.23 = add nsw i32 %24, %add.22
  %25 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 25), align 4
  %add.24 = add nsw i32 %25, %add.23
  %26 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 26), align 8
  %add.25 = add nsw i32 %26, %add.24
  %27 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 27), align 4
  %add.26 = add nsw i32 %27, %add.25
  %28 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 28), align 16
  %add.27 = add nsw i32 %28, %add.26
  %29 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 29), align 4
  %add.28 = add nsw i32 %29, %add.27
  %30 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 30), align 8
  %add.29 = add nsw i32 %30, %add.28
  %31 = load i32, i32* getelementptr inbounds ([32 x i32], [32 x i32]* @arr_i32, i64 0, i64 31), align 4
  %add.30 = add nsw i32 %31, %add.29
  store i32 %add.30, i32* %res, align 16
  ret void
}

declare i32 @foobar(i32) #0

define void @i32_red_call(i32 %val) #0 {
entry:
  %0 = load <8 x i32>, <8 x i32>* bitcast ([32 x i32]* @arr_i32 to <8 x i32>*), align 16
  %add = add nsw i32 undef, undef
  %add.1 = add nsw i32 undef, %add
  %add.2 = add nsw i32 undef, %add.1
  %add.3 = add nsw i32 undef, %add.2
  %add.4 = add nsw i32 undef, %add.3
  %add.5 = add nsw i32 undef, %add.4
  %rdx.shuf = shufflevector <8 x i32> %0, <8 x i32> undef, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 undef, i32 undef, i32 undef, i32 undef>
  %bin.rdx = add nsw <8 x i32> %0, %rdx.shuf
  %rdx.shuf1 = shufflevector <8 x i32> %bin.rdx, <8 x i32> undef, <8 x i32> <i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %bin.rdx2 = add nsw <8 x i32> %bin.rdx, %rdx.shuf1
  %rdx.shuf3 = shufflevector <8 x i32> %bin.rdx2, <8 x i32> undef, <8 x i32> <i32 1, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %bin.rdx4 = add nsw <8 x i32> %bin.rdx2, %rdx.shuf3
  %1 = extractelement <8 x i32> %bin.rdx4, i32 0
  %add.6 = add nsw i32 undef, %add.5
  %res = call i32 @foobar(i32 %1)
  ret void
}

define void @i32_red_invoke(i32 %val) #0 personality i32 (...)* @__gxx_personality_v0 {
entry:
  %0 = load <8 x i32>, <8 x i32>* bitcast ([32 x i32]* @arr_i32 to <8 x i32>*), align 16
  %add = add nsw i32 undef, undef
  %add.1 = add nsw i32 undef, %add
  %add.2 = add nsw i32 undef, %add.1
  %add.3 = add nsw i32 undef, %add.2
  %add.4 = add nsw i32 undef, %add.3
  %add.5 = add nsw i32 undef, %add.4
  %rdx.shuf = shufflevector <8 x i32> %0, <8 x i32> undef, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 undef, i32 undef, i32 undef, i32 undef>
  %bin.rdx = add nsw <8 x i32> %0, %rdx.shuf
  %rdx.shuf1 = shufflevector <8 x i32> %bin.rdx, <8 x i32> undef, <8 x i32> <i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %bin.rdx2 = add nsw <8 x i32> %bin.rdx, %rdx.shuf1
  %rdx.shuf3 = shufflevector <8 x i32> %bin.rdx2, <8 x i32> undef, <8 x i32> <i32 1, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %bin.rdx4 = add nsw <8 x i32> %bin.rdx2, %rdx.shuf3
  %1 = extractelement <8 x i32> %bin.rdx4, i32 0
  %add.6 = add nsw i32 undef, %add.5
  %res = invoke i32 @foobar(i32 %1)
          to label %normal unwind label %exception

exception:                                        ; preds = %entry
  %cleanup = landingpad i8
          cleanup
  br label %normal

normal:                                           ; preds = %exception, %entry
  ret void
}

declare i32 @__gxx_personality_v0(...) #0

attributes #0 = { "target-cpu"="corei7-avx" }
